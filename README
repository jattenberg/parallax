Parallax Machine Learning System  v1.0

This draft: April 28, 2012
© Data Scientists LLC.  All rights reserved.

The goal of this document is to describe at a high level Data Scientists LLC’s Parallax Machine Learning System. This software was designed and optimized exclusively for the objective of providing a scalable, flexible, and embeddable solution to predictive modeling problems in industry. Priority has been given to ensure that all components are embeddable in a variety of production systems.  The system is written in Java; interaction with non-java architectures is achievable either through restful web calls or via thrift. 
Main Ideas and Principles:

complex feature creation; transforming raw data into descriptive numerical examples for consumption of machine learning systems, forcing the complexity of the model into the description of the problem space.
sequentially trainable machine learning models, capable of handling extremely large data sets and data streams, fast, large-scale training, high-throughput testing.
regularization: many different regularization techniques are available, and importantly can be mixed and matched, controlling the complexity of the model, and preventing overfitting.
model selection, hyper-parameter tuning, finding the best model from many candidate models. 
evaluation: a rich set of evaluation metrics are generated, enabling the assessment of the performance of predictive systems.
configurability: machine learning systems can be designed by specifying the layout of the above components in a yaml file. This allows non-programmers to build production quality systems. 

Feature creation: perhaps the most crucial component of any machine learning system is how data is consumed, cleaned, filtered, and transformed into numerical arrays typically taken as input by the core algorithms themselves. The DSI machine learning package has employed functional programming to enable seamless composition of transforms, filters, and projections, in addition to a flexible system for allowing various input channels. A good number of common data transforms are currently implemented, allowing raw data to be transformed into complex, expressive “feature vectors”. Additionally, filtering components are in place to remove data instances not satisfying user-defined rules (including a set common, predefined rules), or specific feature values can be removed according to automatic or user-defined criteria (for instance, values that may be considered outliers). Feature selection may also be employed, selecting only those features believed to be most valuable.

A wide variety of data projections are also available, transforming input from one space to another. This may reduce data to a more manageable size, remove correlated features, or create new features that offer additional predictive power. Data transforms can be performed in sequence or in parallel, to be combined later. This allows multiple data strategies to be used together. Finally, it should be noted that the functional implementation is very efficient. Great care has been taken to ensure object creation has been minimized, and there is never more than the absolute minimum amount of data in memory. 

Predictive Modeling: While a variety of machine learning techniques have been implemented, special emphasis has been given to technique that allow sequential training. While such techniques occasionally lack the predictive complexity of “batch only” models, sequential training facilitates training on arbitrarily large datasets. A number of efficient, well-tested models have been implemented, often with many configurable elements, potentially offering greater predictive power if properly set. Models can be serialized in a number of ways for later use. Among the techniques implemented are regularized cost-sensitive logistic regression, maximum margin linear methods including support vector machines and confidence-weighted passive aggressive models, online kernel-methods, and a rich naive bayes suite capable of modeling likelihoods with a variety of parametric and nonparametric distributions as well as using mixture models for likelihood estimates. 

Generalized optimization procedures are implemented, facilitating the training of models that optimize user-defined, task-specific loss functions (rather than, for example, simply maximizing likelihood), offering a greater degree of flexibility that would typically be possible for many machine learning systems.  Further, in some cases the objectives of several models can be combined into one meta-objective.
 
Ensemble methods are also included, whereby several models are combined into a single aggregate model, potentially offering greater predictive power than would be possible using any single model.

Finally, note that while not currently included in the Parallax software system, a large set of machine learning models are implemented based on the Parallax modeling substrate, including rich decision trees, nearest neighbor models,  gaussian processes, and radial basis function networks. These models could be introduced into Parallax should sufficient demand exist. 

Regularization: Modern data modeling increasingly employs regularization (complexity control based trading off data fitting and simplicity) as a means of reducing errors due to overfitting training data. The DSI machine learning system provides a wide variety of regularization techniques: Lp norms including the popular L2 norm (ridge regression), L1 norm (lasso), probabilistic priors including Gaussian and Cauchy priors, and model entropy. These techniques can be combined in something akin to a super Elastic-Net and applied together, potentially offering benefits that would not be achievable otherwise. 

Model Selection: Often there are multiple candidate models for solving a problem. Furthermore, a given model may have many different parameter settings that can be tuned--boolean, integer, floating point, categorical, and object-valued parameters that affect how the modelling procedure learns from data and how the model makes predictions. The DSI Parallax machine learning system has a parallelizable system for finding the best models and parameter settings from the space of viable candidates. This requires exploring the space of possibilities, often using complex cross-validation to avoid making predictions on the training data, and keeping the choices with the greatest promise. 

Evaluation: Crucial to any machine learning system is detailed knowledge of how well the system is behaving. DSI collects a wide variety of metrics to allow understanding of a system’s and model’s performance, and facilitate choices that yield improvements. 

Configurability: Entire machine learning systems can easily be configured using the yaml markup language. This choice allows even non-programmers to build and tune powerful machine learning systems, once Parallax is integrated. Tweaks can easily be made by making small changes to configuration files. More complex objects are serialized separately, and loaded based on requests specified in in the yaml file. 